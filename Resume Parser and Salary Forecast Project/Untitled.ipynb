{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Skills -- \n",
    "Roles\n",
    "Experience (role, timeline)\n",
    "Education and Certifications\n",
    "Name\n",
    "Email, phone, Linkedin, Github, (RE) [Monday included]\n",
    "\n",
    "# Step 2\n",
    "Role - Ideal Resume (Skills, Experience) ## Make on own\n",
    "\n",
    "# Step 3\n",
    "Match based on the percentage (>60%)\n",
    "Classify based on (Skills, Experience) to Job role\n",
    "\n",
    "# Step 4\n",
    "Recommend Courses/Skills required for the desired role.\n",
    "\n",
    "# Step 5\n",
    "Forecasting role/salary after 10 years based on courses/skills.\n",
    "Breakdown - Students, Experienced(Switching Career), Upgrade(from current role)\n",
    "Link - https://www.payscale.com/research/IN/Job=Software_Engineer/Salary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Used\n",
    "\n",
    "https://omkarpathak.in/2018/12/18/writing-your-own-resume-parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in /home/sai-girish/anaconda3/lib/python3.7/site-packages (20200517)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in /home/sai-girish/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: sortedcontainers in /home/sai-girish/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (2.1.0)\n",
      "Requirement already satisfied: pycryptodome in /home/sai-girish/anaconda3/lib/python3.7/site-packages (from pdfminer.six) (3.9.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "# calling above function and extracting text\n",
    "text = ''\n",
    "for page in extract_text_from_pdf('Elliot-Alderson-Resume.pdf'):\n",
    "    text += ' ' + page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" ELLIOT ALDERSON\\n\\nSoftware Developer\\n\\nSAN FRANCISCO, CA, 32222, UNITED STATES\\n\\n890-555-0401\\n\\nDETAILS\\n\\nPRO FIL E\\n\\n143 Main Ave, San Francisco, CA,\\n32222, United States\\n890-555-0401\\nrozenboomchantal@gmail.com\\n\\nDATE / PLACE OF BIRTH\\n\\n05/10/1985\\nSan Francisco, CA\\n\\nNATIONALITY\\n\\nUSA\\n\\nFull\\n\\nDRIVING LICENSE\\n\\nSKILLS\\n\\nMongoDB\\n\\nExpress.JS\\n\\nAngularJS\\n\\nMode.JS\\n\\nSwift\\n\\nJava\\n\\nPython\\n\\nPassionate Software Engineer with 5 years of professional experience building web applications.\\nProficient in full-stack development, particularly the MEAN stack. \\n\\nEMPL O YMENT  HIS T O RY\\n\\nSoftware Developer at Johnson & Johnson, San Francisco, CA\\n\\nNovember 2015 â€“ November 2017\\n\\nJohnson & Johnson is a Fortune 500 Medical Device and Manufacturing company in the US. As a\\nSoftware Developer, I work on their eCommerce platform in an Agile environment. My daily\\nresponsibilities include: \\n\\nParticipating in daily stand up meetings, led by our Scrum Master\\nUtilizing the MEAN stack to enhance and maintain our eCommerce platform\\n\\nConducting code peer reviews with other members in my team\\nParticipating in product demos \\nDocumenting all code changes, following J&J's change protocols\\n\\nSoftware Developer at PIH Unlimited, San Francisco, CA\\n\\nMay 2014 â€“ November 2015\\n\\nAs a Software Developer at PIH Unlimited, I worked on a small Agile team in a startup environment to\\nprototype and build mobile applications. My daily responsibilities included:\\n\\nBrainstorming with team members to come up with new mobile application concepts\\nWorking with stakeholders to gather functional and technical requirements\\nCreating wireframes and prototypes to test our ideas\\nWriting code to develop iOS and Android applications, primarily using Java and Swift\\nParticipating in MVP and product demos\\nUtilizing automated and manual methods to test our code\\n\\nFacilitating releases of software upgrades \\n\\nIT Intern at Fidelity National Financial, San Francisco, CA\\n\\nJanuary 2012 â€“ May 2014\\n\\nAt Fidelity National Financial, I participated in an IT internship, during which I rotated between their\\n\\ninfrastructure, data analytics, and software engineering departments. My daily activities included:\\n\\nShadowing senior team members to get a feel for their day-to-day responsabilities\\n\\nTaking on small software development projects then presenting my work to the leadership team\\n\\nAssisting with process improvements, making suggestions on workflow changes where needed\\n\\nParticipating in weekly meetings with the entire internship team\\n\\nSan Francisco State University, San Francisco, CA\\n\\nEDU CAT IO N\\n\\nNovember 2015\\n\\nDegree: BS in Computer Science\\n\\n\\x0c \\x0c\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = ' '.join([text.lstrip() for text in text.split('\\n')])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', None, pattern)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAN FRANCISCO\n",
      "UNITED STATES\n",
      "PRO FIL\n",
      "Main Ave\n",
      "San Francisco\n",
      "United States\n",
      "San Francisco\n",
      "Passionate Software\n",
      "Software Engineer\n",
      "San Francisco\n",
      "Scrum Master\n",
      "PIH Unlimited\n",
      "San Francisco\n",
      "PIH Unlimited\n",
      "Fidelity National\n",
      "National Financial\n",
      "San Francisco\n",
      "Fidelity National\n",
      "National Financial\n",
      "San Francisco\n",
      "Francisco State\n",
      "State University\n",
      "San Francisco\n",
      "Computer Science\n"
     ]
    }
   ],
   "source": [
    "extract_name(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "    \n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5550401\n"
     ]
    }
   ],
   "source": [
    "mobile_number = extract_mobile_number(text)\n",
    "print(mobile_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rozenboomchantal@gmail.com\n"
     ]
    }
   ],
   "source": [
    "email = extract_email(text)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Skills\n",
    "\n",
    "Created skills.csv file with \"machine learning,ml,artificial intelligence,ai,natural language processing,nlp\" data, Add more relevant skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(ISC)2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>.NET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>.NET CLR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>.NET Compact Framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>.NET Framework</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    skill\n",
       "0                  (ISC)2\n",
       "1                    .NET\n",
       "2                .NET CLR\n",
       "3  .NET Compact Framework\n",
       "4          .NET Framework"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_dataset = pd.read_csv('linkedin_skill.txt', sep='\\n', header=None)\n",
    "skills_dataset.columns = ['skill']\n",
    "skills_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zoominfo',\n",
       " 'zoomtext',\n",
       " 'zoomerang',\n",
       " 'zope',\n",
       " 'zotero',\n",
       " 'zsh',\n",
       " 'zuken',\n",
       " 'zultys',\n",
       " 'zulu education products',\n",
       " 'zuludesk',\n",
       " 'zumba',\n",
       " 'zumba instruction',\n",
       " 'zuora',\n",
       " 'zymography',\n",
       " 'zynx',\n",
       " 'zyxel',\n",
       " 'z/os',\n",
       " 'z/vm',\n",
       " 'zlinux',\n",
       " 'zseries']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = []\n",
    "for i in range(len(skills_dataset)):\n",
    "    skills.append(str(skills_dataset.loc[i, 'skill']).lower())\n",
    "skills[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Demos', 'Ios', 'Pro', 'Leadership', 'Conducting', 'Io', 'Web', 'Analytics', 'Android', 'Infrastructure', 'Java', 'Led', 'Writing', 'Python', 'Manufacturing', 'Ecommerce', 'O', 'Suggestions', 'Application', 'Scrum', 'Shadowing', 'Reviews', 'Building', 'Mobile applications', 'Data analytics', 'Birth', 'N', 'Angularjs', 'Prototype', 'Swift', 'Computer science', 'Agile', 'Mobile', 'Workflow', 'Mongodb', 'Express']\n"
     ]
    }
   ],
   "source": [
    "skills = extract_skills(text)\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-69f55ad683d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meducation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_education\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meducation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-2332e1f8cc86>\u001b[0m in \u001b[0;36mextract_education\u001b[0;34m(resume_text)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[?|$|.|!|,]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mEDUCATION\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtex\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0medu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnlp_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Extract year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "education = extract_education(text)\n",
    "print(education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'education': [('MS', '2005')], 'email': 'JBWhit@gmail.com', 'mobile_number': '+16509433715', 'skills': ['Python', 'Git']}\n"
     ]
    }
   ],
   "source": [
    "result = {'education': education, 'email': email, 'mobile_number': mobile_number, 'skills': skills}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
