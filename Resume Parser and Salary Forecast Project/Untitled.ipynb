{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Skills -- \n",
    "Roles\n",
    "Experience (role, timeline)\n",
    "Education and Certifications\n",
    "Name\n",
    "Email, phone, Linkedin, Github, (RE) [Monday included]\n",
    "\n",
    "# Step 2\n",
    "Role - Ideal Resume (Skills, Experience) ## Make on own\n",
    "\n",
    "# Step 3\n",
    "Match based on the percentage (>60%)\n",
    "Classify based on (Skills, Experience) to Job role\n",
    "\n",
    "# Step 4\n",
    "Recommend Courses/Skills required for the desired role.\n",
    "\n",
    "# Step 5\n",
    "Forecasting role/salary after 10 years based on courses/skills.\n",
    "Breakdown - Students, Experienced(Switching Career), Upgrade(from current role)\n",
    "Link - https://www.payscale.com/research/IN/Job=Software_Engineer/Salary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Used\n",
    "\n",
    "https://omkarpathak.in/2018/12/18/writing-your-own-resume-parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as fh:\n",
    "        # iterate over all pages of PDF document\n",
    "        for page in PDFPage.get_pages(fh, caching=True, check_extractable=True):\n",
    "            # creating a resoure manager\n",
    "            resource_manager = PDFResourceManager()\n",
    "            \n",
    "            # create a file handle\n",
    "            fake_file_handle = io.StringIO()\n",
    "            \n",
    "            # creating a text converter object\n",
    "            converter = TextConverter(\n",
    "                                resource_manager, \n",
    "                                fake_file_handle, \n",
    "                                codec='utf-8', \n",
    "                                laparams=LAParams()\n",
    "                        )\n",
    "\n",
    "            # creating a page interpreter\n",
    "            page_interpreter = PDFPageInterpreter(\n",
    "                                resource_manager, \n",
    "                                converter\n",
    "                            )\n",
    "\n",
    "            # process current page\n",
    "            page_interpreter.process_page(page)\n",
    "            \n",
    "            # extract text\n",
    "            text = fake_file_handle.getvalue()\n",
    "            yield text\n",
    "\n",
    "            # close open handles\n",
    "            converter.close()\n",
    "            fake_file_handle.close()\n",
    "\n",
    "# calling above function and extracting text\n",
    "text = ''\n",
    "for page in extract_text_from_pdf('RESUME.pdf'):\n",
    "    text += ' ' + page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 16 February 1999\\n\\nInternship & Projects\\n\\nHackathons\\n\\nApril’20\\n\\nLowe’s Campus Hackathaon\\nBuilt an android application to help the customers find products \\nin the store and help them navigate to the corresponding\\naisle/shelf. If there is a shopping list, provided the best shopping \\ntrip to complete the purchases. \\n\\n                                 TechGig\\n\\nCurrently\\n\\nDeloitte TechnoUtsav 3.0\\nBuilding an android app for a grocery store to recommend the\\nProducts for the customer based on various factors.\\n\\n                 TechGig\\n\\nJune’20       Web Scraping (Internship @ ByteMatics Technologies Pvt. Ltd.)         \\n\\nAs part of the internship, scraped the website \\nhttps://www.rightmove.co.uk/ using scrapy, stored the data in \\nMongodb and created the API using flask server. \\n\\nCurrently     Data Analytics (Internship @ BlitzJobs )\\n\\nAs part of the internship, performed data collection, cleaning,\\nManipulation and analysis of 250 start-ups in india and currently \\nApplying regression models to perform segragation based on \\nVarious parameters. \\n\\nStudents For Students (S4S)\\nThis is an android application which provides a platform for a\\nStudent to sell or buy used books. The application was built using\\nAndroid-Studio and Firebase.\\n\\nCurrently\\n\\nCode Asylums\\n\\nBuilding a ML model to guide students and working professionals\\nTo the right career path based on skill set required for the desired\\nrole/position using data from their CV and predicting their role and\\nSalary in a company down the line, to be deployed at CodeAsylums\\n\\n2014\\n\\n2014\\n\\n2017\\n\\n2013\\n\\nAwarded Rs 5,000  for merit in Kendriya Vidyalaya Ballari for \\nScoring 10.0 CGPA\\n\\nAwarded 15,000 Rs/- by Nalanda Pre University College for being \\nDistrict topper in Nalanda Scholarship Test\\n\\nSince 2016\\n\\nFFE Scholar: Being Awarded 40,000 Rs/- per annum by Foundation\\nFor Excellence for exceptional performance in JEE Mains and \\nPre-University Public examinations of Department of \\nPre-University Education Karnataka\\n\\nBeing Awarded merit cum means scholarship from epass by \\nKarnataka State Government.\\n\\nRepresented Kendriya Vidyalaya Ballari in cricket at regional \\nlevel\\n\\nB.Tech Computer Science Engineering\\n\\nNIT-K | 2021 | CGPA:6.61/10\\n\\nFeb-Apr’19\\n\\nS Sai Girish\\n\\ni\\n\\nT\\n\\nB saigirishgilly98@gmail.com\\n\\n+91 8105556401\\n\\nm github.com/saigirishgilly98\\nEducation\\n\\nClass XII\\n\\nSri Chaitanya Pre-University College\\n| 2016 | 96.67 %\\n\\nClass X\\n\\nKendriya Vidyalaya Bellary\\n| 2014 | CGPA: 10/10\\n\\nSkills\\n\\nWebDev : HTML, CSS\\n\\nOther: Data Analysis, Android App\\nDevelopment, Web Scraping (Scrapy)\\n\\nExtra-Curricular\\n\\n- Volunteer at NITK Film Festival’17\\n- Member at NITK Film Festival’18\\n- Volunteer at NITK Incident’17\\n- Member of Marketing and Publicity\\n  Team of NITK Incident’17\\n- Volunteer at NITK Engi Connect’17\\n- Volunteer at NITK Engi Connect’18\\n\\nLanguages: C++, Python, MySQL\\n\\nAchievements\\n\\n\\x0c'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text = ' '.join([text.lstrip() for text in text.split('\\n')])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# initialize matcher with a vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def extract_name(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    \n",
    "    # First name and Last name are always Proper Nouns\n",
    "    pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "    \n",
    "    matcher.add('NAME', None, pattern)\n",
    "    \n",
    "    matches = matcher(nlp_text)\n",
    "    \n",
    "    for match_id, start, end in matches:\n",
    "        span = nlp_text[start:end]\n",
    "        print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Campus Hackathaon\n",
      "Deloitte TechnoUtsav\n",
      "ByteMatics Technologies\n",
      "Technologies Pvt\n",
      "Data Analytics\n",
      "Code Asylums\n",
      "Kendriya Vidyalaya\n",
      "Vidyalaya Ballari\n",
      "Nalanda Pre\n",
      "Pre University\n",
      "University College\n",
      "Nalanda Scholarship\n",
      "Scholarship Test\n",
      "JEE Mains\n",
      "Pre-\n",
      "-University\n",
      "University Public\n",
      "Pre-\n",
      "-University\n",
      "University Education\n",
      "Education Karnataka\n",
      "Karnataka State\n",
      "State Government\n",
      "Kendriya Vidyalaya\n",
      "Vidyalaya Ballari\n",
      "Computer Science\n",
      "Science Engineering\n",
      "K |\n",
      "S Sai\n",
      "Sai Girish\n",
      "Sri Chaitanya\n",
      "Chaitanya Pre\n",
      "Pre-\n",
      "-University\n",
      "University College\n",
      "Kendriya Vidyalaya\n",
      "Vidyalaya Bellary\n",
      "| CGPA\n",
      "Data Analysis\n",
      "Android App\n",
      "NITK Film\n",
      "Film Festival’17\n",
      "NITK Film\n",
      "Film Festival’18\n",
      "NITK Incident’17\n",
      "NITK Incident’17\n",
      "NITK Engi\n",
      "Engi Connect’17\n",
      "NITK Engi\n"
     ]
    }
   ],
   "source": [
    "extract_name(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Phone Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    flag = 0\n",
    "    phone = re.findall(re.compile(r'(\\d{3}-\\d{3}-\\d{4})'), text)\n",
    "    if len(phone) == 0:\n",
    "        phone = re.findall(re.compile(r'(?:(?:\\+?([1-9]|[0-9][0-9]|[0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|[0-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(\\d+))?'), text)\n",
    "        flag = 1\n",
    "        \n",
    "    if phone:\n",
    "        if flag == 1:\n",
    "            number = ''.join(phone[0])\n",
    "        else:\n",
    "            number = ''.join(phone[0].split('-'))\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+918105556401\n"
     ]
    }
   ],
   "source": [
    "mobile_number = extract_mobile_number(text)\n",
    "print(mobile_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saigirishgilly98@gmail.com\n"
     ]
    }
   ],
   "source": [
    "email = extract_email(text)\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Skills\n",
    "\n",
    "Created skills.csv file with \"machine learning,ml,artificial intelligence,ai,natural language processing,nlp\" data, Add more relevant skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(ISC)2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>.NET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>.NET CLR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>.NET Compact Framework</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>.NET Framework</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    skill\n",
       "0                  (ISC)2\n",
       "1                    .NET\n",
       "2                .NET CLR\n",
       "3  .NET Compact Framework\n",
       "4          .NET Framework"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_dataset = pd.read_csv('linkedin_skill.txt', sep='\\n', header=None)\n",
    "skills_dataset.columns = ['skill']\n",
    "skills_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zoominfo',\n",
       " 'zoomtext',\n",
       " 'zoomerang',\n",
       " 'zope',\n",
       " 'zotero',\n",
       " 'zsh',\n",
       " 'zuken',\n",
       " 'zultys',\n",
       " 'zulu education products',\n",
       " 'zuludesk',\n",
       " 'zumba',\n",
       " 'zumba instruction',\n",
       " 'zuora',\n",
       " 'zymography',\n",
       " 'zynx',\n",
       " 'zyxel',\n",
       " 'z/os',\n",
       " 'z/vm',\n",
       " 'zlinux',\n",
       " 'zseries']"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills = []\n",
    "for i in range(len(skills_dataset)):\n",
    "    skills.append(str(skills_dataset.loc[i, 'skill']).lower())\n",
    "skills[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Studio', 'Ml', 'Data analysis', 'Firebase', 'Webdev', 'Application', 'App', 'Regression models', 'Campus', 'Flask', 'X', 'Html', 'Cricket', 'Public', 'Languages', 'Customer', 'C++', 'Mysql', 'Analytics', 'Building', 'Salary', 'Government', 'Css', 'Publicity', 'Python', 'Ffe', 'Api', 'Web', 'Android', 'Film', 'Store', 'Web scraping', 'Models', 'Shopping', 'Mongodb', 'Foundation', 'Grocery']\n"
     ]
    }
   ],
   "source": [
    "skills = extract_skills(text)\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'XII', 'X', 'PHD'\n",
    "        ]\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.string.strip() for sent in nlp_text.sents]\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                try:\n",
    "                    if index + 1 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += text + nlp_text[index + 1]\n",
    "                        else:\n",
    "                            edu[tex] = text + nlp_text[index + 1]\n",
    "                    if index - 1 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += nlp_text[index - 1]\n",
    "                        else:\n",
    "                            edu[tex] = nlp_text[index - 1]\n",
    "                    if index + 2 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += nlp_text[index + 2]\n",
    "                        else:\n",
    "                            edu[tex] = nlp_text[index + 2]\n",
    "                    if index - 2 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += nlp_text[index - 2] \n",
    "                        else:\n",
    "                            edu[tex] = nlp_text[index - 2]\n",
    "                    if index + 3 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += nlp_text[index + 3] \n",
    "                        else:\n",
    "                            edu[tex] = nlp_text[index + 3]\n",
    "                    if index - 3 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += nlp_text[index - 3]\n",
    "                        else:\n",
    "                            edu[tex] = nlp_text[index - 3]\n",
    "                    if index - 4 < len(nlp_text):\n",
    "                        if tex in edu:\n",
    "                            edu[tex] += nlp_text[index - 4]\n",
    "                        else:\n",
    "                            edu[tex] = nlp_text[index - 4]\n",
    "                except:\n",
    "                    edu[tex] = text\n",
    "    print(edu)\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BTech': 'B.TechComputer Science EngineeringRepresented Kendriya Vidyalaya Ballari in cricket at regional \\nlevelNIT-K | 2021Being Awarded merit cum means scholarship from epass by \\nKarnataka State Government.| CGPA:6.61/10For Excellence for exceptional performance in JEE Mains and \\nPre-University Public examinations of Department of \\nPre-University Education KarnatakaSince 2016\\n\\nFFE Scholar: Being Awarded 40,000 Rs/- per annum by Foundation', 'XII': 'Class XIISrim github.com/saigirishgilly98\\nEducationChaitanya Pre-University College8105556401| 2016 | 96.67 %\\n\\nClass X\\n\\nKendriya Vidyalaya Bellary\\n| 2014 | CGPA: 10/10+91saigirishgilly98@gmail.com', 'X': '| 2016 | 96.67 %\\n\\nClass X\\n\\nKendriya Vidyalaya Bellary\\n| 2014 | CGPA: 10/10Skills\\n\\nWebDev : HTML, CSSChaitanya Pre-University CollegeOther:SriData Analysis, Android App\\nDevelopment, Web Scraping (Scrapy)Class XIIm github.com/saigirishgilly98\\nEducation'}\n",
      "[('BTech', '2021'), ('XII', '2016'), ('X', '2016')]\n"
     ]
    }
   ],
   "source": [
    "education = extract_education(text)\n",
    "print(education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'education': [('BTech', '2021'), ('XII', '2016'), ('X', '2016')], 'email': 'saigirishgilly98@gmail.com', 'mobile_number': '+918105556401', 'skills': ['Studio', 'Ml', 'Data analysis', 'Firebase', 'Webdev', 'Application', 'App', 'Regression models', 'Campus', 'Flask', 'X', 'Html', 'Cricket', 'Public', 'Languages', 'Customer', 'C++', 'Mysql', 'Analytics', 'Building', 'Salary', 'Government', 'Css', 'Publicity', 'Python', 'Ffe', 'Api', 'Web', 'Android', 'Film', 'Store', 'Web scraping', 'Models', 'Shopping', 'Mongodb', 'Foundation', 'Grocery']}\n"
     ]
    }
   ],
   "source": [
    "result = {'education': education, 'email': email, 'mobile_number': mobile_number, 'skills': skills}\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
